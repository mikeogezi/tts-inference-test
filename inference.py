# -*- coding: utf-8 -*-
'''Test Tacotron2 WaveGlow TTS engine'''

'''will load the WaveGlow model pre-trained on [LJ Speech dataset](https://keithito.com/LJ-Speech-Dataset/)

### Model Description

The Tacotron 2 and WaveGlow model form a text-to-speech system that enables user to synthesise a natural sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model (also available via torch.hub) produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech.

### Example

In the example below:
- pretrained Tacotron2 and Waveglow models are loaded from torch.hub
- Tacotron2 generates mel spectrogram given tensor represantation of an input text ("Hello world, I missed you")
- Waveglow generates sound given the mel spectrogram
- the output sound is saved in an 'audio.wav' file

To run the example you need some extra python packages installed.
These are needed for preprocessing the text and audio, as well as for display and input / output.

pip install numpy scipy librosa unidecode inflect librosa
'''

import torch
import os
from keras.utils.data_utils import get_file
from IPython.display import display, Audio

import sys
sys.path.append(os.path.abspath('.'))
sys.dont_write_bytecode = True

import numpy as np
from scipy.io.wavfile import write
from keras.utils.data_utils import get_file
try:
  from google.colab import drive
  root_dir = '/content/tts-inference-test/'
  # drive.mount('/content/gdrive')
  # root_dir = '/content/gdrive/My Drive'
except ImportError as ex:
  root_dir = os.path.expandvars('$HOME/Desktop/programming/ml/pytorch/waveglow/tts-inference-test/')

device = 'cpu'
fp16 = False
trained_dir = os.path.join(root_dir, 'trained_models')
if fp16:
  waveglow_path = os.path.join(trained_dir, 'joc-waveglow-fp16-pyt-20190306')
  tacotron2_path = os.path.join(trained_dir, 'joc-tacotron2-fp16-pyt-20190306')
  if not os.path.exists(waveglow_path):
    get_file(waveglow_path, 'https://developer.nvidia.com/joc-waveglow-fp16-pyt-20190306')
  if not os.path.exists(tacotron2_path):
    get_file(tacotron2_path, 'https://developer.nvidia.com/joc-tacotron2-fp16-pyt-20190306')
else:
  waveglow_path = os.path.join(trained_dir, 'joc-waveglow-fp32-pyt-20190306')
  tacotron2_path = os.path.join(trained_dir, 'joc-tacotron2-fp32-pyt-20190306')
  if not os.path.exists(waveglow_path):
    get_file(waveglow_path, 'https://developer.nvidia.com/joc-waveglow-fp32-pyt-20190306')
  if not os.path.exists(tacotron2_path):
    get_file(tacotron2_path, 'https://developer.nvidia.com/joc-tacotron2-fp32-pyt-20190306')


# from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/inference.py
def checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.
    :param state_dict: model's state dict
    """
    ret = False
    for key, _ in state_dict.items():
        if key.find('module.') != -1:
            ret = True
            break
    return ret


# from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/inference.py
def unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.
    :param state_dict: model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace('module.1.', '')
        new_key = new_key.replace('module.', '')
        new_state_dict[new_key] = value
    return new_state_dict


def load_tacotron2 (fp16=fp16, device=device):
  '''Constructs a Tacotron 2 model (nn.module with additional infer(input) method).
    For detailed information on model input and output, training recipies, inference and performance
    visit: github.com/NVIDIA/DeepLearningExamples and/or ngc.nvidia.com

    Args (type[, default value]):
        pretrained (bool, True): If True, returns a model pretrained on LJ Speech dataset.
        model_math (str, 'fp32'): returns a model in given precision ('fp32' or 'fp16')
        n_symbols (int, 148): Number of symbols used in a sequence passed to the prenet, see
                              https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/text/symbols.py
        p_attention_dropout (float, 0.1): dropout probability on attention LSTM (1st LSTM layer in decoder)
        p_decoder_dropout (float, 0.1): dropout probability on decoder LSTM (2nd LSTM layer in decoder)
        max_decoder_steps (int, 1000): maximum number of generated mel spectrograms during inference
  '''
  from PyTorch.SpeechSynthesis.Tacotron2.tacotron2 import model as tacotron2
  from PyTorch.SpeechSynthesis.Tacotron2.models import batchnorm_to_float, lstmcell_to_float
  from PyTorch.SpeechSynthesis.Tacotron2.tacotron2.text import text_to_sequence

  ckpt_file = tacotron2_path
  ckpt = torch.load(ckpt_file, map_location=device)
  
  state_dict = ckpt['state_dict']
  if checkpoint_from_distributed(state_dict):
    state_dict = unwrap_distributed(state_dict)
  config = ckpt['config']

  m = tacotron2.Tacotron2(**config, device=device)

  if fp16:
    m = batchnorm_to_float(m.half())
    m = lstmcell_to_float(m)

  m.load_state_dict(state_dict)
  m.text_to_sequence = text_to_sequence

  return m


def load_waveglow (fp16=fp16):
  '''Constructs a WaveGlow model (nn.module with additional infer(input) method).
    For detailed information on model input and output, training recipies, inference and performance
    visit: github.com/NVIDIA/DeepLearningExamples and/or ngc.nvidia.com

    Args:
        pretrained (bool): If True, returns a model pretrained on LJ Speech dataset.
        model_math (str, 'fp32'): returns a model in given precision ('fp32' or 'fp16')
  '''
  from PyTorch.SpeechSynthesis.Tacotron2.waveglow import model as waveglow
  from PyTorch.SpeechSynthesis.Tacotron2.models import batchnorm_to_float

  ckpt_file = waveglow_path
  ckpt = torch.load(ckpt_file, map_location=device)
  
  state_dict = ckpt['state_dict']
  if checkpoint_from_distributed(state_dict):
    state_dict = unwrap_distributed(state_dict)
  config = ckpt['config']

  m = waveglow.WaveGlow(**config)

  if fp16:
    m = batchnorm_to_float(m.half())
    for mat in m.convinv:
      mat.float()

  m.load_state_dict(state_dict)

  return m


'''Prepare the waveglow model for inference'''
waveglow = load_waveglow()
if not fp16:
  waveglow = waveglow.remove_weightnorm(waveglow)
waveglow = waveglow.to(device)
waveglow.eval()

'''Load tacotron2'''
tacotron2 = load_tacotron2(device=device)
tacotron2 = tacotron2.to(device)
tacotron2.eval()

sentences = [
  # From July 8, 2017 New York Times:
  'Scientists at the CERN laboratory say they have discovered a new particle.',
  'There\'s a way to measure the acute emotional intelligence that has never gone out of style.',
  'President Trump met with other leaders at the Group of 20 conference.',
  'The Senate\'s bill to repeal and replace the Affordable Care Act is now imperiled.',
  # From Google's Tacotron example page:
  'Generative adversarial network or variational auto-encoder.',
  'Basilar membrane and otolaryngology are not auto-correlations.',
  'He has read the whole thing.',
  'He reads books.',
  'He thought it was time to present the present.',
  'Thisss isrealy awhsome.',
  'Punctuation sensitivity, is working.',
  'Punctuation sensitivity is working.',
  "Peter Piper picked a peck of pickled peppers. How many pickled peppers did Peter Piper pick?",
  "She sells sea-shells on the sea-shore. The shells she sells are sea-shells I'm sure.",
  "Tajima Airport serves Toyooka.",
  # A final Thank you note!
  'Thank you so much for your support!',
]

'''Now, let's make the model synthesize some speech'''
'''Now chain pre-processing -> tacotron2 -> waveglow'''
for i, sent in enumerate(sentences):
  sequence = np.array(tacotron2.text_to_sequence(sent, ['english_cleaners']))[None, :]
  sequence = torch.from_numpy(sequence).to(device=device, dtype=torch.int64)

  print 'Synthesizing audio for Sentence {i}'.format(i=i)
  print 'Sentence: `{sent}`'.format(sent=sent)
  # run the models
  with torch.no_grad():
      _, mel, _, _ = tacotron2.infer(sequence)
      audio = waveglow.infer(mel)
  audio_numpy = audio[0].data.cpu().numpy()
  rate = 22050

  '''You can write it to a file and listen to it'''
  write("audio_{i}.wav".format(i=i), rate, audio_numpy)

  '''Alternatively, play it right away in a notebook with IPython widgets'''
  from IPython.display import Audio
  Audio(audio_numpy, rate=rate)